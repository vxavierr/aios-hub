# Story CL2-018: Implementation Tasks IM-001 to IM-007

**Story ID:** CL2-018
**Epic:** EPIC-CLONE-LAB-V2
**Phase:** 2 - Validation
**Status:** Ready
**Points:** 5
**Author:** Morgan (PM)

---

## Description

Implementar as 7 Validation Tasks de Implementacao (IM-001 a IM-007), responsaveis por validar aspectos praticos e tecnicos da implementacao do clone, incluindo prompts, manifest e deploy.

### Problem Statement
A implementacao do clone requer validacao de aspectos tecnicos para garantir que o clone pode ser efetivamente usado. Estas 7 tasks validam a prontidao para implementacao.

---

## Acceptance Criteria

```gherkin
Given synthesized personality DNA
When implementation tasks are executed
Then all 7 tasks return valid TaskResults

Given generated prompts
When IM-002 (Prompt Quality) runs
Then it validates prompt effectiveness

Given system manifest
When IM-004 (Manifest Validity) runs
Then it validates manifest completeness and structure

Given deployment configuration
When IM-006 (Deployment Readiness) runs
Then it validates deployment readiness
```

---

## Technical Requirements

### Task Definitions

| Task ID | Name | Description |
|---------|------|-------------|
| IM-001 | Prompt Structure | Valida estrutura dos prompts gerados |
| IM-002 | Prompt Quality | Valida qualidade e efetividade dos prompts |
| IM-003 | Token Efficiency | Valida eficiencia de tokens |
| IM-004 | Manifest Validity | Valida manifesto gerado |
| IM-005 | API Compatibility | Valida compatibilidade com APIs |
| IM-006 | Deployment Readiness | Valida prontidao para deploy |
| IM-007 | Integration Test | Valida testes de integracao |

### Implementation Example

```typescript
// packages/validation/src/tasks/implementation/im-002-prompt-quality.ts

import { IValidationTask, TaskContext, TaskResult, TaskDefinition } from '../task.interface';

export class IM002PromptQuality implements IValidationTask {
  readonly definition: TaskDefinition = {
    id: 'IM-002',
    category: 'implementation',
    name: 'Prompt Quality',
    description: 'Validates the quality and effectiveness of generated prompts',
    requiredInputs: ['extractedData', 'dna', 'prompts'],
    outputs: ['qualityScore', 'issues', 'recommendations'],
    timeout: 60000,
    retryable: true,
    priority: 'high'
  };

  async execute(context: TaskContext): Promise<TaskResult> {
    const startTime = Date.now();
    const findings: TaskFinding[] = [];

    const prompts = context.options.prompts as string[];
    if (!prompts || prompts.length === 0) {
      return {
        taskId: this.definition.id,
        status: 'failed',
        score: 0,
        confidence: 0,
        findings: [{ type: 'issue', description: 'No prompts provided', severity: 'critical' }],
        recommendations: ['Generate prompts before validation'],
        executionTime: Date.now() - startTime,
        metadata: {}
      };
    }

    const qualityAnalysis = await this.analyzePromptQuality(prompts, context);

    // Check for common prompt issues
    if (qualityAnalysis.tooLongCount > 0) {
      findings.push({
        type: 'warning',
        description: `${qualityAnalysis.tooLongCount} prompts may be too long for optimal performance`,
        severity: 'medium'
      });
    }

    if (qualityAnalysis.ambiguousCount > 0) {
      findings.push({
        type: 'warning',
        description: `${qualityAnalysis.ambiguousCount} prompts contain ambiguous instructions`,
        severity: 'medium'
      });
    }

    if (qualityAnalysis.missingContextCount > 0) {
      findings.push({
        type: 'issue',
        description: `${qualityAnalysis.missingContextCount} prompts lack necessary context`,
        severity: 'high'
      });
    }

    const score = this.calculateScore(qualityAnalysis);

    return {
      taskId: this.definition.id,
      status: score >= 60 ? 'passed' : 'failed',
      score,
      confidence: qualityAnalysis.confidence,
      findings,
      recommendations: this.generateRecommendations(qualityAnalysis),
      executionTime: Date.now() - startTime,
      metadata: {
        promptsAnalyzed: prompts.length,
        averageLength: qualityAnalysis.averageLength,
        qualityBreakdown: qualityAnalysis.breakdown
      }
    };
  }

  private async analyzePromptQuality(prompts: string[], context: TaskContext): Promise<QualityAnalysis> {
    const analysis: QualityAnalysis = {
      tooLongCount: 0,
      ambiguousCount: 0,
      missingContextCount: 0,
      averageLength: 0,
      breakdown: {
        clarity: 0,
        completeness: 0,
        specificity: 0,
        structure: 0
      },
      confidence: 0
    };

    let totalLength = 0;

    for (const prompt of prompts) {
      totalLength += prompt.length;

      // Check length (rough token estimate)
      if (prompt.length > 4000) {
        analysis.tooLongCount++;
      }

      // Check for ambiguous phrases
      const ambiguousPhrases = ['maybe', 'perhaps', 'might', 'could be', 'sort of', 'kind of'];
      if (ambiguousPhrases.some(phrase => prompt.toLowerCase().includes(phrase))) {
        analysis.ambiguousCount++;
      }

      // Check for missing context indicators
      if (!prompt.includes('context') && !prompt.includes('background') && prompt.length > 500) {
        analysis.missingContextCount++;
      }

      // Analyze individual prompt quality
      const promptQuality = this.analyzeIndividualPrompt(prompt);
      analysis.breakdown.clarity += promptQuality.clarity;
      analysis.breakdown.completeness += promptQuality.completeness;
      analysis.breakdown.specificity += promptQuality.specificity;
      analysis.breakdown.structure += promptQuality.structure;
    }

    // Average the breakdown scores
    const numPrompts = prompts.length;
    analysis.breakdown.clarity /= numPrompts;
    analysis.breakdown.completeness /= numPrompts;
    analysis.breakdown.specificity /= numPrompts;
    analysis.breakdown.structure /= numPrompts;

    analysis.averageLength = totalLength / numPrompts;
    analysis.confidence = this.calculateConfidence(analysis, numPrompts);

    return analysis;
  }

  private analyzeIndividualPrompt(prompt: string): PromptQuality {
    const quality: PromptQuality = {
      clarity: 100,
      completeness: 100,
      specificity: 100,
      structure: 100
    };

    // Clarity checks
    if (prompt.includes('...') || prompt.includes('etc')) {
      quality.clarity -= 10;
    }

    // Completeness checks
    if (!prompt.includes(':') && prompt.length > 200) {
      quality.completeness -= 15; // No clear sections
    }

    // Specificity checks
    const vagueWords = ['things', 'stuff', 'something', 'anything'];
    const vagueCount = vagueWords.filter(w => prompt.toLowerCase().includes(w)).length;
    quality.specificity -= vagueCount * 10;

    // Structure checks
    const hasSections = prompt.includes('\n\n') || prompt.includes('##');
    const hasNumbering = /\d+\./.test(prompt);
    if (!hasSections && !hasNumbering && prompt.length > 500) {
      quality.structure -= 20;
    }

    return quality;
  }

  private calculateScore(analysis: QualityAnalysis): number {
    const breakdownAvg = (
      analysis.breakdown.clarity +
      analysis.breakdown.completeness +
      analysis.breakdown.specificity +
      analysis.breakdown.structure
    ) / 4;

    // Penalty for issues
    let penalty = 0;
    penalty += analysis.tooLongCount * 3;
    penalty += analysis.ambiguousCount * 5;
    penalty += analysis.missingContextCount * 10;

    return Math.max(0, Math.min(100, Math.round(breakdownAvg - penalty)));
  }

  async validateInputs(context: TaskContext): Promise<boolean> {
    return context.options.prompts !== undefined;
  }

  canExecute(context: TaskContext): boolean {
    return context.previousResults.has('SY-007'); // DNA Completeness
  }

  getDependencies(): TaskId[] {
    return ['SY-007', 'SY-008']; // DNA Completeness and Synthesis Coherence
  }
}
```

### File Structure

```
packages/validation/src/tasks/implementation/
├── im-001-prompt-structure.ts
├── im-002-prompt-quality.ts
├── im-003-token-efficiency.ts
├── im-004-manifest-validity.ts
├── im-005-api-compatibility.ts
├── im-006-deployment-readiness.ts
├── im-007-integration-test.ts
├── index.ts
└── __tests__/
    └── implementation-tasks.test.ts
```

---

## Business Value

As Implementation Tasks garantem que o clone esta **pronto para uso pratico**, validando prompts, manifesto e configuracoes de deploy.

**Beneficios:**
- Valida qualidade dos prompts gerados
- Garante compatibilidade com APIs
- Verifica prontidao para deploy
- Reduz falhas em producao

---

## Risks

| Risk | Probability | Impact | Mitigation |
|------|-------------|--------|------------|
| Prompts muito longos | Medium | Medium | Validacao de tokens |
| Incompatibilidade de API | Low | High | Testes de compatibilidade |
| Falhas de deploy | Medium | High | Checklist de deploy |

---

## Scope

### In Scope
- All 7 implementation task implementations
- Unit tests for each task
- Prompt quality validation logic
- Finding and recommendation generation

### Out of Scope
- Other task categories
- Task Registry & Executor
- CLI integration

---

## Dependencies

| Dependency | Type | Status |
|------------|------|--------|
| CL2-012 (Task Interface) | Package | Pending |
| CL2-017 (Synthesis Tasks) | Package | Pending |
| @clone-lab/core (DNA, Manifest) | Package | Available |

---

## Dev Notes

- Use token counting for efficiency validation
- Consider LLM-based prompt quality scoring
- Add API-specific compatibility checks
- Implement deployment checklist validation

---

## File List

| File | Action | Status |
|------|--------|--------|
| `packages/validation/src/tasks/implementation/im-001-prompt-structure.ts` | Create | Pending |
| `packages/validation/src/tasks/implementation/im-002-prompt-quality.ts` | Create | Pending |
| `packages/validation/src/tasks/implementation/im-003-token-efficiency.ts` | Create | Pending |
| `packages/validation/src/tasks/implementation/im-004-manifest-validity.ts` | Create | Pending |
| `packages/validation/src/tasks/implementation/im-005-api-compatibility.ts` | Create | Pending |
| `packages/validation/src/tasks/implementation/im-006-deployment-readiness.ts` | Create | Pending |
| `packages/validation/src/tasks/implementation/im-007-integration-test.ts` | Create | Pending |
| `packages/validation/src/tasks/implementation/index.ts` | Create | Pending |
| `packages/validation/src/tasks/implementation/__tests__/implementation-tasks.test.ts` | Create | Pending |

---

## Definition of Done

- [ ] All 7 tasks implement IValidationTask correctly
- [ ] Each task returns valid TaskResult
- [ ] Prompt quality is accurately assessed
- [ ] Findings are descriptive and actionable
- [ ] Unit tests pass (coverage > 80%)
- [ ] Handles edge cases gracefully
- [ ] Lint passa sem erros
- [ ] Code review aprovado

---

## QA Checklist

- [ ] All 7 tasks execute without errors
- [ ] Task results are consistent
- [ ] Quality assessments are accurate
- [ ] Deployment readiness is properly validated
- [ ] Edge cases handled

---

## Change Log

| Date | Author | Description |
|------|--------|-------------|
| 2026-02-20 | Morgan (PM) | Story created |

---

*Story generated by Morgan (PM Agent) - AIOS Framework*
