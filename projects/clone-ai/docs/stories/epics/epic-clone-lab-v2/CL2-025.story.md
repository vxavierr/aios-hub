# Story CL2-025: CP-4 Prompt Testing Checkpoint

**Story ID:** CL2-025
**Epic:** EPIC-CLONE-LAB-V2
**Phase:** 2 - Validation
**Status:** Ready
**Points:** 3
**Author:** Morgan (PM)

---

## Description

Implementar o **Checkpoint 4 (CP-4)** - Prompt Testing, que valida os prompts gerados atraves de testes automatizados antes de liberar o clone para uso.

### Problem Statement
Os prompts gerados a partir do DNA precisam ser testados para garantir que produzem respostas condizentes com a personalidade sendo clonada.

---

## Acceptance Criteria

```gherkin
Given generated prompts from approved DNA
When CP-4 evaluates the prompts
Then it runs test scenarios and assesses response quality

Given prompts with low fidelity responses
When CP-4 evaluates
Then it identifies issues and recommends prompt adjustments

Given test responses meeting quality thresholds
When CP-4 evaluates
Then it can approve for clone deployment

Given CP-4 evaluation complete
When human reviews
Then clear test results and recommendations are available
```

---

## Technical Requirements

### Checkpoint Implementation

```typescript
// packages/validation/src/checkpoints/cp-4-prompt-testing.ts

import { ICheckpoint, CheckpointDefinition, CheckpointContext, CheckpointResult, CheckpointDetail } from './checkpoint.interface';

export class CP4PromptTesting implements ICheckpoint {
  readonly definition: CheckpointDefinition = {
    id: 'CP-4',
    name: 'Prompt Testing',
    description: 'Validates generated prompts through automated testing before clone deployment',
    triggerPhase: 'manifest',
    requiredApprovals: 1,
    autoApproveConditions: [
      'testPassRate >= 85',
      'averageFidelityScore >= 75',
      'noCriticalFailures',
      'responseConsistency >= 80'
    ],
    timeout: 2 * 60 * 60 * 1000  // 2 hours
  };

  async evaluate(context: CheckpointContext): Promise<CheckpointResult> {
    const details: CheckpointDetail[] = [];
    const recommendations: string[] = [];

    const testResponses = context.options.testResponses as TestResponse[];
    const prompts = context.options.prompts as string[];

    if (!testResponses || testResponses.length === 0) {
      return {
        checkpointId: this.definition.id,
        status: 'failed',
        score: 0,
        summary: 'No test responses available',
        details: [{
          category: 'Test Data',
          status: 'fail',
          message: 'No test responses have been generated'
        }],
        recommendations: ['Run prompt tests before this checkpoint'],
        timestamp: new Date()
      };
    }

    // 1. Test Coverage
    const coverageAssessment = this.assessTestCoverage(testResponses);
    details.push({
      category: 'Test Coverage',
      status: coverageAssessment.status,
      message: coverageAssessment.message,
      evidence: `Scenarios tested: ${coverageAssessment.scenariosTested}/${coverageAssessment.totalScenarios}`
    });

    // 2. Response Fidelity
    const fidelityAssessment = this.assessResponseFidelity(testResponses, context);
    details.push({
      category: 'Response Fidelity',
      status: fidelityAssessment.status,
      message: fidelityAssessment.message,
      evidence: `Average fidelity: ${fidelityAssessment.averageFidelity}%`
    });

    // 3. Voice Consistency
    const voiceAssessment = this.assessVoiceConsistency(testResponses);
    details.push({
      category: 'Voice Consistency',
      status: voiceAssessment.status,
      message: voiceAssessment.message,
      evidence: `Voice match score: ${voiceAssessment.voiceMatchScore}%`
    });

    // 4. Response Consistency
    const consistencyAssessment = this.assessResponseConsistency(testResponses);
    details.push({
      category: 'Response Consistency',
      status: consistencyAssessment.status,
      message: consistencyAssessment.message,
      evidence: `Consistency score: ${consistencyAssessment.consistencyScore}%`
    });

    // 5. Edge Case Handling
    const edgeCaseAssessment = this.assessEdgeCaseHandling(testResponses);
    details.push({
      category: 'Edge Case Handling',
      status: edgeCaseAssessment.status,
      message: edgeCaseAssessment.message,
      evidence: `Edge cases passed: ${edgeCaseAssessment.passed}/${edgeCaseAssessment.total}`
    });

    // 6. Prompt Quality
    const promptQualityAssessment = this.assessPromptQuality(prompts, context);
    details.push({
      category: 'Prompt Quality',
      status: promptQualityAssessment.status,
      message: promptQualityAssessment.message,
      evidence: `Prompt quality score: ${promptQualityAssessment.qualityScore}%`
    });

    // Calculate overall score
    const score = this.calculateScore([
      coverageAssessment,
      fidelityAssessment,
      voiceAssessment,
      consistencyAssessment,
      edgeCaseAssessment,
      promptQualityAssessment
    ]);

    // Generate recommendations
    if (fidelityAssessment.averageFidelity < 70) {
      recommendations.push('Review prompts to improve fidelity - responses may not match expected personality');
    }
    if (voiceAssessment.voiceMatchScore < 70) {
      recommendations.push('Adjust voice parameters in prompts - tone may not match expected style');
    }
    if (consistencyAssessment.consistencyScore < 70) {
      recommendations.push('Review prompts for consistency - responses vary too much');
    }
    if (edgeCaseAssessment.passed < edgeCaseAssessment.total * 0.8) {
      recommendations.push('Improve edge case handling in prompts');
    }

    const hasFail = details.some(d => d.status === 'fail');
    const hasWarn = details.some(d => d.status === 'warn');

    return {
      checkpointId: this.definition.id,
      status: hasFail ? 'waiting_approval' : (hasWarn ? 'waiting_approval' : 'pending'),
      score,
      summary: this.generateSummary(score, details),
      details,
      recommendations,
      timestamp: new Date()
    };
  }

  canAutoApprove(result: CheckpointResult): boolean {
    const hasFail = result.details.some(d => d.status === 'fail');
    return result.score >= 80 && !hasFail;
  }

  async getReviewData(context: CheckpointContext): Promise<Record<string, unknown>> {
    const testResponses = context.options.testResponses as TestResponse[] || [];

    return {
      testScenarios: this.getTestScenarioSummaries(testResponses),
      fidelityBreakdown: this.getFidelityBreakdown(testResponses),
      sampleResponses: this.getSampleResponses(testResponses, 5),
      failedTests: this.getFailedTests(testResponses),
      promptAnalysis: this.getPromptAnalysis(context)
    };
  }

  validateApproval(approval: ApprovalInfo): boolean {
    return !!approval.approvedBy && !!approval.approvedAt;
  }

  private assessTestCoverage(responses: TestResponse[]): AssessmentResult {
    const expectedScenarios = [
      'greeting', 'question', 'opinion', 'advice', 'storytelling',
      'disagreement', 'humor', 'expertise', 'personal', 'edge_case'
    ];

    const testedScenarios = new Set(responses.map(r => r.scenario));
    const scenariosTested = testedScenarios.size;
    const totalScenarios = expectedScenarios.length;
    const coverage = scenariosTested / totalScenarios;

    return {
      status: coverage >= 0.8 ? 'pass' : (coverage >= 0.5 ? 'warn' : 'fail'),
      message: coverage >= 0.8
        ? 'Test coverage is comprehensive'
        : 'Test coverage is insufficient',
      scenariosTested,
      totalScenarios,
      score: coverage * 100
    };
  }

  private assessResponseFidelity(responses: TestResponse[], context: CheckpointContext): AssessmentResult {
    const fidelityScores = responses.map(r => r.fidelityScore || 0);
    const averageFidelity = fidelityScores.length > 0
      ? fidelityScores.reduce((a, b) => a + b) / fidelityScores.length
      : 0;

    // Get comparison data from QA tasks
    const qaResults = this.getResultsByPrefix(context.taskResults, 'QA-');
    const fidelityResult = qaResults.find(r => r.taskId === 'QA-001');
    const qaFidelity = fidelityResult?.score || 0;

    const combinedFidelity = (averageFidelity + qaFidelity) / 2;

    return {
      status: combinedFidelity >= 70 ? 'pass' : (combinedFidelity >= 50 ? 'warn' : 'fail'),
      message: combinedFidelity >= 70
        ? 'Response fidelity is good'
        : 'Response fidelity needs improvement',
      averageFidelity: Math.round(combinedFidelity),
      score: combinedFidelity
    };
  }

  private assessVoiceConsistency(responses: TestResponse[]): AssessmentResult {
    // Analyze voice characteristics across responses
    const voiceScores = responses.map(r => r.voiceMatchScore || 0);
    const voiceMatchScore = voiceScores.length > 0
      ? voiceScores.reduce((a, b) => a + b) / voiceScores.length
      : 0;

    // Check variance
    const variance = this.calculateVariance(voiceScores);
    const consistencyPenalty = Math.min(20, variance / 5);

    const adjustedScore = Math.max(0, voiceMatchScore - consistencyPenalty);

    return {
      status: adjustedScore >= 70 ? 'pass' : (adjustedScore >= 50 ? 'warn' : 'fail'),
      message: adjustedScore >= 70
        ? 'Voice is consistent across responses'
        : 'Voice consistency needs improvement',
      voiceMatchScore: Math.round(adjustedScore),
      score: adjustedScore
    };
  }

  private assessResponseConsistency(responses: TestResponse[]): AssessmentResult {
    // Check for similar questions getting similar answers
    const consistencyScores = responses.map(r => r.consistencyScore || 0);
    const consistencyScore = consistencyScores.length > 0
      ? consistencyScores.reduce((a, b) => a + b) / consistencyScores.length
      : 0;

    return {
      status: consistencyScore >= 70 ? 'pass' : (consistencyScore >= 50 ? 'warn' : 'fail'),
      message: consistencyScore >= 70
        ? 'Responses are consistent'
        : 'Response consistency needs improvement',
      consistencyScore: Math.round(consistencyScore),
      score: consistencyScore
    };
  }

  private assessEdgeCaseHandling(responses: TestResponse[]): AssessmentResult {
    const edgeCases = responses.filter(r => r.scenario === 'edge_case' || r.isEdgeCase);
    const passed = edgeCases.filter(r => r.passed !== false).length;
    const total = edgeCases.length || 1;

    const passRate = passed / total;

    return {
      status: passRate >= 0.8 ? 'pass' : (passRate >= 0.5 ? 'warn' : 'fail'),
      message: passRate >= 0.8
        ? 'Edge cases handled well'
        : 'Edge case handling needs improvement',
      passed,
      total: edgeCases.length,
      score: passRate * 100
    };
  }

  private assessPromptQuality(prompts: string[] | undefined, context: CheckpointContext): AssessmentResult {
    if (!prompts || prompts.length === 0) {
      // Get from implementation tasks
      const imResults = this.getResultsByPrefix(context.taskResults, 'IM-');
      const qualityResult = imResults.find(r => r.taskId === 'IM-002');
      return {
        status: qualityResult?.status === 'passed' ? 'pass' : 'warn',
        message: qualityResult?.status === 'passed'
          ? 'Prompts passed quality checks'
          : 'Prompt quality assessment pending',
        qualityScore: qualityResult?.score || 50,
        score: qualityResult?.score || 50
      };
    }

    const avgLength = prompts.reduce((sum, p) => sum + p.length, 0) / prompts.length;
    const hasStructure = prompts.every(p => p.includes(':') || p.includes('\n\n'));
    const hasContext = prompts.every(p => p.length > 200);

    let qualityScore = 50;
    if (avgLength >= 500 && avgLength <= 4000) qualityScore += 20;
    if (hasStructure) qualityScore += 15;
    if (hasContext) qualityScore += 15;

    return {
      status: qualityScore >= 70 ? 'pass' : (qualityScore >= 50 ? 'warn' : 'fail'),
      message: qualityScore >= 70
        ? 'Prompts are well structured'
        : 'Prompt quality needs improvement',
      qualityScore,
      score: qualityScore
    };
  }

  private calculateScore(assessments: AssessmentResult[]): number {
    return Math.round(
      assessments.reduce((sum, a) => sum + a.score, 0) / assessments.length
    );
  }

  private calculateVariance(values: number[]): number {
    if (values.length === 0) return 0;
    const mean = values.reduce((a, b) => a + b) / values.length;
    return values.reduce((sum, v) => sum + Math.pow(v - mean, 2), 0) / values.length;
  }

  private generateSummary(score: number, details: CheckpointDetail[]): string {
    const status = score >= 80 ? 'PASSED' : (score >= 60 ? 'CONDITIONAL PASS' : 'NEEDS IMPROVEMENT');
    const issues = details.filter(d => d.status === 'fail').length;
    const warnings = details.filter(d => d.status === 'warn').length;

    return `Prompt Testing: ${status} (Score: ${score}%). ${issues} failures, ${warnings} warnings.`;
  }

  private getResultsByPrefix(results: Map<TaskId, TaskResult>, prefix: string): TaskResult[] {
    const filtered: TaskResult[] = [];
    for (const [taskId, result] of results) {
      if (taskId.startsWith(prefix)) {
        filtered.push(result);
      }
    }
    return filtered;
  }

  private getTestScenarioSummaries(responses: TestResponse[]): unknown[] {
    return responses.map(r => ({
      scenario: r.scenario,
      passed: r.passed,
      fidelityScore: r.fidelityScore
    }));
  }

  private getFidelityBreakdown(responses: TestResponse[]): Record<string, number> {
    const breakdown: Record<string, number[]> = {};
    for (const r of responses) {
      if (!breakdown[r.scenario]) breakdown[r.scenario] = [];
      breakdown[r.scenario].push(r.fidelityScore || 0);
    }
    const result: Record<string, number> = {};
    for (const [scenario, scores] of Object.entries(breakdown)) {
      result[scenario] = scores.reduce((a, b) => a + b) / scores.length;
    }
    return result;
  }

  private getSampleResponses(responses: TestResponse[], count: number): unknown[] {
    return responses.slice(0, count).map(r => ({
      prompt: r.prompt?.substring(0, 100),
      response: r.response?.substring(0, 200),
      fidelityScore: r.fidelityScore
    }));
  }

  private getFailedTests(responses: TestResponse[]): unknown[] {
    return responses.filter(r => r.passed === false).map(r => ({
      scenario: r.scenario,
      issue: r.issue
    }));
  }

  private getPromptAnalysis(context: CheckpointContext): unknown {
    return {};
  }
}

interface TestResponse {
  scenario: string;
  prompt?: string;
  response?: string;
  fidelityScore?: number;
  voiceMatchScore?: number;
  consistencyScore?: number;
  passed?: boolean;
  isEdgeCase?: boolean;
  issue?: string;
}

interface AssessmentResult {
  status: 'pass' | 'warn' | 'fail';
  message: string;
  score: number;
  [key: string]: unknown;
}
```

### File Structure

```
packages/validation/src/checkpoints/
├── cp-4-prompt-testing.ts
└── __tests__/
    └── cp-4-prompt-testing.test.ts
```

---

## Business Value

CP-4 garante que os **prompts funcionam** antes de liberar o clone, testando respostas em varios cenarios.

**Beneficios:**
- Valida prompts antes do deploy
- Identifica problemas de fidelidade
- Testa consistencia de respostas
- Verifica tratamento de edge cases

---

## Risks

| Risk | Probability | Impact | Mitigation |
|------|-------------|--------|------------|
| Testes insuficientes | Medium | High | Cobertura minima |
| Falsos positivos | Medium | Medium | Multi-metric scoring |
| Tempo de teste | Low | Low | Parallel testing |

---

## Scope

### In Scope
- CP-4 checkpoint implementation
- Test coverage assessment
- Response fidelity evaluation
- Unit tests

### Out of Scope
- Other checkpoints
- Test generation
- CLI integration

---

## Dependencies

| Dependency | Type | Status |
|------------|------|--------|
| CL2-018 (Implementation Tasks) | Package | Pending |
| CL2-019 (Quality Tasks) | Package | Pending |
| CL2-021 (Checkpoint Interface) | Package | Pending |
| CL2-024 (CP-3) | Package | Pending |

---

## Dev Notes

- Integrate with test framework
- Add LLM-based fidelity scoring
- Consider A/B testing for prompt variants

---

## File List

| File | Action | Status |
|------|--------|--------|
| `packages/validation/src/checkpoints/cp-4-prompt-testing.ts` | Create | Pending |
| `packages/validation/src/checkpoints/__tests__/cp-4-prompt-testing.test.ts` | Create | Pending |

---

## Definition of Done

- [ ] Implements ICheckpoint correctly
- [ ] Evaluates test responses
- [ ] Returns valid CheckpointResult
- [ ] Auto-approve logic works
- [ ] Unit tests pass (coverage > 80%)
- [ ] Lint passa sem erros
- [ ] Code review aprovado

---

## QA Checklist

- [ ] Evaluation returns correct status
- [ ] Score calculation is accurate
- [ ] Failed tests identified
- [ ] Recommendations are helpful
- [ ] Edge cases handled

---

## Change Log

| Date | Author | Description |
|------|--------|-------------|
| 2026-02-20 | Morgan (PM) | Story created |

---

*Story generated by Morgan (PM Agent) - AIOS Framework*
